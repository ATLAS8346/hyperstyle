{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uuviq3qQkUFy"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# os.chdir('/content')\n",
    "# CODE_DIR = 'hyperstyle'\n",
    "CODE_DIR = '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QQ6XEmlHlXbk"
   },
   "outputs": [],
   "source": [
    "# !git clone https://github.com/yuval-alaluf/hyperstyle.git $CODE_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JaRUFuVHkzye"
   },
   "outputs": [],
   "source": [
    "# !wget https://github.com/ninja-build/ninja/releases/download/v1.8.2/ninja-linux.zip\n",
    "# !sudo unzip ninja-linux.zip -d /usr/local/bin/\n",
    "# !sudo update-alternatives --install /usr/bin/ninja ninja /usr/local/bin/ninja 1 --force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "23baccYQlU9E"
   },
   "outputs": [],
   "source": [
    "os.chdir(f'./{CODE_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d13v7In0kTJn"
   },
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import pprint\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "sys.path.append(\".\")\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from models.stylegan2.model import Generator\n",
    "from utils.common import tensor2im\n",
    "from utils.inference_utils import run_inversion\n",
    "from utils.domain_adaptation_utils import run_domain_adaptation\n",
    "from utils.model_utils import load_model, load_generator\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HRjtz6uLkTJs"
   },
   "source": [
    "## Step 1: Select Experiment Type\n",
    "Select which experiment you wish to perform inference on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XESWAO65kTJt"
   },
   "outputs": [],
   "source": [
    "#@title Select which experiment you wish to perform inference on: { run: \"auto\" }\n",
    "experiment_type = 'ffhq_hypernet' #@param ['ffhq_hypernet', 'cars_hypernet', 'afhq_wild_hypernet']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4etDz82xkTJz"
   },
   "source": [
    "## Step 2: Prepare to Download Pretrained Models \n",
    "As part of this repository, we provide pretrained models for each of the above experiments. Here, we'll create the download command needed for downloading the desired model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KSnjlBZOkTJ0"
   },
   "outputs": [],
   "source": [
    "def get_download_model_command(file_id, file_name):\n",
    "    \"\"\" Get wget download command for downloading the desired model and save to directory ../pretrained_models. \"\"\"\n",
    "    current_directory = os.getcwd()\n",
    "    save_path = os.path.join(os.path.dirname(current_directory), CODE_DIR, \"pretrained_models\")\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    command = f\"gdown --id {file_id} -O {save_path}/{file_name}\"\n",
    "    return command    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m4sjldFMkTJ5"
   },
   "outputs": [],
   "source": [
    "MODEL_PATHS = {\n",
    "    \"ffhq_hypernet\": {\"id\": \"1C3dEIIH1y8w1-zQMCyx7rDF0ndswSXh4\", \"name\": \"hyperstyle_ffhq.pt\"},\n",
    "    \"cars_hypernet\": {\"id\": \"1WZ7iNv5ENmxXFn6dzPeue1jQGNp6Nr9d\", \"name\": \"hyperstyle_cars.pt\"},\n",
    "    \"afhq_wild_hypernet\": {\"id\": \"1OMAKYRp3T6wzGr0s3887rQK-5XHlJ2gp\", \"name\": \"hyperstyle_afhq_wild.pt\"}\n",
    "}\n",
    "path = MODEL_PATHS[experiment_type]\n",
    "hyperstyle_download_command = get_download_model_command(file_id=path[\"id\"], file_name=path[\"name\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_ENCODERS_PATHS = {\n",
    "    \"ffhq_hypernet\": {\"id\": \"1M-hsL3W_cJKs77xM1mwq2e9-J0_m7rHP\", \"name\": \"faces_w_encoder.pt\"},\n",
    "    \"cars_hypernet\": {\"id\": \"1GZke8pfXMSZM9mfT-AbP1Csyddf5fas7\", \"name\": \"cars_w_encoder.pt\"},\n",
    "    \"afhq_wild_hypernet\": {\"id\": \"1MhEHGgkTpnTanIwuHYv46i6MJeet2Nlr\", \"name\": \"afhq_wild_w_encoder.pt\"}\n",
    "}\n",
    "path = W_ENCODERS_PATHS[experiment_type]\n",
    "w_encoder_download_command = get_download_model_command(file_id=path[\"id\"], file_name=path[\"name\"]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Tozsg81kTKA",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Step 3: Define Inference Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XIhyc7RqkTKB"
   },
   "source": [
    "Below we have a dictionary defining parameters such as the path to the pretrained model to use and the path to the image to perform inference on.  \n",
    "While we provide default values to run this script, feel free to change as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2kE5y1-skTKC",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "EXPERIMENT_DATA_ARGS = {\n",
    "    \"ffhq_hypernet\": {\n",
    "#         \"model_path\": \"./pretrained_models/hyperstyle_ffhq.pt\",\n",
    "#         \"w_encoder_path\": \"./pretrained_models/faces_w_encoder.pt\",\n",
    "#         \"image_path\": \"./notebooks/images/face_image.jpg\",\n",
    "        \"model_path\": \"../pretrained_models/hyperstyle_ffhq.pt\",\n",
    "        \"w_encoder_path\": \"../pretrained_models/faces_w_encoder.pt\",\n",
    "        \"image_path\": \"../notebooks/images/face_image.jpg\",\n",
    "        \"transform\": transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
    "    },\n",
    "    \"cars_hypernet\": {\n",
    "#         \"model_path\": \"./pretrained_models/hyperstyle_cars.pt\",\n",
    "#         \"w_encoder_path\": \"./pretrained_models/cars_w_encoder.pt\",\n",
    "#         \"image_path\": \"./notebooks/images/car_image.jpg\",\n",
    "        \"model_path\": \"../pretrained_models/hyperstyle_cars.pt\",\n",
    "        \"w_encoder_path\": \"../pretrained_models/cars_w_encoder.pt\",\n",
    "        \"image_path\": \"../notebooks/images/car_image.jpg\",\n",
    "        \"transform\": transforms.Compose([\n",
    "            transforms.Resize((192, 256)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
    "    },\n",
    "    \"afhq_wild_hypernet\": {\n",
    "#         \"model_path\": \"./pretrained_models/hyperstyle_afhq_wild.pt\",\n",
    "#         \"w_encoder_path\": \"./pretrained_models/afhq_wild_w_encoder.pt\",\n",
    "#         \"image_path\": \"./notebooks/images/afhq_wild_img.jpg\",\n",
    "        \"model_path\": \"../pretrained_models/hyperstyle_afhq_wild.pt\",\n",
    "        \"w_encoder_path\": \"../pretrained_models/afhq_wild_w_encoder.pt\",\n",
    "        \"image_path\": \"../notebooks/images/afhq_wild_image.jpg\",\n",
    "        \"transform\": transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
    "    },\n",
    "    \"domain_adaptation\": {  # used in a later part of the notebook, checkpoint path will be defined separately\n",
    "        \"image_path\": \"../notebooks/images/domain_adaptation.jpg\",\n",
    "        \"transform\": transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IzUHoD9ukTKG"
   },
   "outputs": [],
   "source": [
    "EXPERIMENT_ARGS = EXPERIMENT_DATA_ARGS[experiment_type]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To reduce the number of requests to fetch the model, we'll check if the model was previously downloaded and saved before downloading the model.  \n",
    "We'll download the model for the selected experiment and save it to the folder `../pretrained_models`.\n",
    "\n",
    "We also need to verify that the model was downloaded correctly. All of our models should weigh approximately 1.3GB.\n",
    "Note that if the file weighs several KBs, you most likely encounter a \"quota exceeded\" error from Google Drive. In that case, you should try downloading the model again after a few hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jQ31J_m7kTJ8",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(EXPERIMENT_ARGS['model_path']) or os.path.getsize(EXPERIMENT_ARGS['model_path']) < 1000000:\n",
    "    print(f'Downloading HyperStyle model for {experiment_type}...')\n",
    "    os.system(hyperstyle_download_command)\n",
    "    # if google drive receives too many requests, we'll reach the quota limit and be unable to download the model\n",
    "    if os.path.getsize(EXPERIMENT_ARGS['model_path']) < 1000000:\n",
    "        raise ValueError(\"Pretrained model was unable to be downloaded correctly!\")\n",
    "    else:\n",
    "        print('Done.')\n",
    "else:\n",
    "    print(f'HyperStyle model for {experiment_type} already exists!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, we need to download the WEncoder for the desired domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(EXPERIMENT_ARGS['w_encoder_path']) or os.path.getsize(EXPERIMENT_ARGS['w_encoder_path']) < 1000000:\n",
    "    print(f'Downloading the WEncoder model for {experiment_type}...')\n",
    "    os.system(w_encoder_download_command)\n",
    "    # if google drive receives too many requests, we'll reach the quota limit and be unable to download the model\n",
    "    if os.path.getsize(EXPERIMENT_ARGS['w_encoder_path']) < 1000000:\n",
    "        raise ValueError(\"Pretrained model was unable to be downloaded correctly!\")\n",
    "    else:\n",
    "        print('Done.')\n",
    "else:\n",
    "    print(f'WEncoder model for {experiment_type} already exists!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TAWrUehTkTKJ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Step 4: Load Pretrained Model\n",
    "We assume that you have downloaded all relevant models and placed them in the directory defined by the above dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1t-AOhP1kTKJ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_path = EXPERIMENT_ARGS['model_path']\n",
    "net, opts = load_model(model_path, update_opts={\"w_encoder_checkpoint_path\": EXPERIMENT_ARGS['w_encoder_path']})\n",
    "print('Model successfully loaded!')\n",
    "pprint.pprint(vars(opts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4weLFoPbkTKZ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Step 5: Visualize Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r2H9zFLJkTKa",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "image_path = EXPERIMENT_DATA_ARGS[experiment_type][\"image_path\"]\n",
    "original_image = Image.open(image_path).convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-lbLKtl-kTKc"
   },
   "outputs": [],
   "source": [
    "if experiment_type == 'cars_encode':\n",
    "    original_image = original_image.resize((192, 256))\n",
    "else:\n",
    "    original_image = original_image.resize((256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "original_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o6oqf8JwzK0K",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Align Image\n",
    "\n",
    "Note: in this notebook we'll run alignment on the input image when working on the human facial domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hJ9Ce1aYzmFF"
   },
   "outputs": [],
   "source": [
    "def run_alignment(image_path):\n",
    "    import dlib\n",
    "    from scripts.align_faces_parallel import align_face\n",
    "    if not os.path.exists(\"shape_predictor_68_face_landmarks.dat\"):\n",
    "        print('Downloading files for aligning face image...')\n",
    "        os.system('wget http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2')\n",
    "        os.system('bzip2 -dk shape_predictor_68_face_landmarks.dat.bz2')\n",
    "        print('Done.')\n",
    "    predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "    aligned_image = align_face(filepath=image_path, predictor=predictor) \n",
    "    print(\"Aligned image has shape: {}\".format(aligned_image.size))\n",
    "    return aligned_image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aTZcKMdK8y77"
   },
   "outputs": [],
   "source": [
    "input_is_aligned = False\n",
    "if experiment_type == \"ffhq_hypernet\" and not input_is_aligned:\n",
    "    input_image = run_alignment(image_path)\n",
    "else:\n",
    "    input_image = original_image\n",
    "\n",
    "input_image.resize((256, 256))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D0BmXzu1kTKg"
   },
   "source": [
    "## Step 6: Perform Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T3h3E7VLkTKg"
   },
   "outputs": [],
   "source": [
    "img_transforms = EXPERIMENT_ARGS['transform']\n",
    "transformed_image = img_transforms(input_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M5eWR2S4OSDM",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we'll run inference. By default, we'll run using 5 inference steps. You can change the parameter in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ct_jm0obOSDM",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "opts.n_iters_per_batch = 5\n",
    "opts.resize_outputs = False  # generate outputs at full resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ls5zb0fRkTKs"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    tic = time.time()\n",
    "    result_batch, result_latents, _ = run_inversion(transformed_image.unsqueeze(0).cuda(), \n",
    "                                                    net, \n",
    "                                                    opts,\n",
    "                                                    return_intermediate_results=True)\n",
    "    toc = time.time()\n",
    "    print('Inference took {:.4f} seconds.'.format(toc - tic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nq0dkSz6kTKv"
   },
   "source": [
    "### Visualize Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UVR03XT_kTK0",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We'll visualize the step-by-step outputs side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ca5BtxdUOSDN"
   },
   "outputs": [],
   "source": [
    "if opts.dataset_type == \"cars_encode\":\n",
    "    resize_amount = (256, 192) if opts.resize_outputs else (512, 384)\n",
    "else:\n",
    "    resize_amount = (256, 256) if opts.resize_outputs else (opts.output_size, opts.output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WdR51hOROSDN",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_coupled_results(result_batch, transformed_image):\n",
    "    result_tensors = result_batch[0]  # there's one image in our batch\n",
    "    final_rec = tensor2im(result_tensors[-1]).resize(resize_amount)\n",
    "    input_im = tensor2im(transformed_image).resize(resize_amount)\n",
    "    res = np.concatenate([np.array(input_im), np.array(final_rec)], axis=1)\n",
    "    res = Image.fromarray(res)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uSDCvtTMOSDN"
   },
   "source": [
    "Note that the step-by-step outputs are shown left-to-right with the original input on the right-hand side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lb3raAKFOSDN"
   },
   "outputs": [],
   "source": [
    "res = get_coupled_results(result_batch, transformed_image)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qaB7RN7cOSDN"
   },
   "outputs": [],
   "source": [
    "# save image \n",
    "outputs_path = \"./outputs\"\n",
    "os.makedirs(outputs_path, exist_ok=True)\n",
    "res.save(os.path.join(outputs_path, os.path.basename(image_path)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ISEMFxmekTK7",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Domain Adaptation\n",
    "\n",
    "In the paper, we show that the weight offsets predicted by HyperStyle over the FFHQ domain are also applicable on fine-tuned generators such as toonify and StyleGAN-NADA.\n",
    "\n",
    "\n",
    "We demonstrate this idea below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FKbAFK7_OSDO"
   },
   "outputs": [],
   "source": [
    "generator_type = 'toonify' #@param ['toonify', 'pixar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download fine-tuned generator\n",
    "FINETUNED_MODELS = {\n",
    "    \"toonify\": {'id': '1r3XVCt_WYUKFZFxhNH-xO2dTtF6B5szu', 'name': 'toonify.pt'},\n",
    "    \"pixar\": {'id': '1trPW-To9L63x5gaXrbAIPkOU0q9f_h05', 'name': 'pixar.pt'},\n",
    "    \"sketch\": {'id': '1aHhzmxT7eD90txAN93zCl8o9CUVbMFnD', 'name': 'sketch.pt'},\n",
    "    \"disney_princess\": {'id': '1rXHZu4Vd0l_KCiCxGbwL9Xtka7n3S2NB', 'name': 'disney_princess.pt'}\n",
    "}\n",
    "EXPERIMENT_ARGS = EXPERIMENT_DATA_ARGS['domain_adaptation']\n",
    "path = FINETUNED_MODELS[generator_type]\n",
    "\n",
    "# generator_path = os.path.join(\"./pretrained_models\", path['name'])\n",
    "generator_path = os.path.join(\"../pretrained_models\", path['name'])\n",
    "\n",
    "if not os.path.exists(generator_path):\n",
    "    print(f'Downloading fine-tuned {generator_type} generator...')\n",
    "    download_command = get_download_model_command(file_id=path[\"id\"], file_name=path[\"name\"])\n",
    "    os.system(download_command)\n",
    "    print('Done.')\n",
    "else:\n",
    "    print(f'Fine-tuned {generator_type} generator already exists!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K3v0X3ZWkTK8",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# load model\n",
    "fine_tuned_generator = load_generator(generator_path)\n",
    "print(f'Fine-tuned {generator_type} generator successfully loaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# load ReStyle e4e:\n",
    "RESTYLE_E4E_MODELS = {'id': '1e2oXVeBPXMQoUoC_4TNwAWpOPpSEhE_e', 'name': 'restlye_e4e.pt'}\n",
    "\n",
    "# restyle_e4e_path = os.path.join(\"./pretrained_models\", RESTYLE_E4E_MODELS['name'])\n",
    "restyle_e4e_path = os.path.join(\"../pretrained_models\", RESTYLE_E4E_MODELS['name'])\n",
    "\n",
    "if not os.path.exists(restyle_e4e_path):\n",
    "    print('Downloading ReStyle-e4e model...')\n",
    "    download_command = get_download_model_command(file_id=RESTYLE_E4E_MODELS[\"id\"], file_name=RESTYLE_E4E_MODELS[\"name\"])\n",
    "    os.system(download_command)\n",
    "    print('Done.')\n",
    "else:\n",
    "    print('ReStyle-e4e model already exists!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load restyle-e4e model\n",
    "restyle_e4e, restyle_e4e_opts = load_model(restyle_e4e_path, is_restyle_encoder=True)\n",
    "print(f'ReStyle-e4e model successfully loaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XW-CJsuwOSDO"
   },
   "outputs": [],
   "source": [
    "# load image. Note that uploaded images must be aligned first, example image is already aligned.\n",
    "image_path = EXPERIMENT_DATA_ARGS['domain_adaptation'][\"image_path\"]\n",
    "input_is_aligned = True\n",
    "if not input_is_aligned:\n",
    "    input_image = run_alignment(image_path)\n",
    "else:\n",
    "    input_image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "input_image.resize((256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MmPWPODaOSDP"
   },
   "outputs": [],
   "source": [
    "# transform image\n",
    "img_transforms = EXPERIMENT_ARGS['transform']\n",
    "transformed_image = img_transforms(input_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BiMjTyMzOSDP"
   },
   "outputs": [],
   "source": [
    "restyle_e4e_opts.n_iters_per_batch = 5\n",
    "restyle_e4e_opts.resize_outputs = False\n",
    "opts.n_iters_per_batch = 5\n",
    "opts.resize_outputs = False  # generate outputs at full resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o81i-MtOOSDQ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    tic = time.time()\n",
    "    result, _ = run_domain_adaptation(transformed_image.unsqueeze(0).cuda(), \n",
    "                                      net, \n",
    "                                      opts, \n",
    "                                      fine_tuned_generator, \n",
    "                                      restyle_e4e, \n",
    "                                      restyle_e4e_opts)\n",
    "    toc = time.time()\n",
    "    print('Inference took {:.4f} seconds.'.format(toc - tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_res = tensor2im(result[0]).resize(resize_amount)\n",
    "input_im = tensor2im(transformed_image).resize(resize_amount)\n",
    "res = np.concatenate([np.array(input_im), np.array(final_res)], axis=1)\n",
    "res = Image.fromarray(res)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save image \n",
    "outputs_path = f\"./outputs/domain_adaptation/{generator_type}\"\n",
    "os.makedirs(outputs_path, exist_ok=True)\n",
    "res.save(os.path.join(outputs_path, os.path.basename(image_path)))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "inference_playground.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}